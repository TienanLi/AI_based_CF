{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from tianshou.policy import DDPGPolicy\n",
    "from tianshou.utils.net.common import Net\n",
    "from tianshou.exploration import GaussianNoise\n",
    "from tianshou.trainer import offpolicy_trainer\n",
    "from tianshou.utils.net.continuous import Actor, Critic\n",
    "from tianshou.data import Collector, ReplayBuffer, VectorReplayBuffer, to_numpy\n",
    "import torch\n",
    "import datetime\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observations shape: (3,)\n",
      "Actions shape: (1,)\n",
      "liming\n"
     ]
    }
   ],
   "source": [
    "from CF_env import CFEnv\n",
    "\n",
    "usedRewardFunction = 'liming'\n",
    "\n",
    "env = CFEnv(usedRewardFunction)\n",
    "train_envs = CFEnv(usedRewardFunction)\n",
    "test_envs = CFEnv(usedRewardFunction)\n",
    "\n",
    "\n",
    "state_shape = env.observation_space.shape \n",
    "action_shape = env.action_space.shape\n",
    "print(\"Observations shape:\", state_shape)\n",
    "print(\"Actions shape:\", action_shape)\n",
    "print(env.rewardName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #1: 10001it [02:09, 77.47it/s, env_step=10000, len=1614, loss/actor=7421.031, loss/critic=427594.035, n/ep=0, n/st=1, rew=-3175363.81]                           \n",
      "Epoch #2:   0%|          | 15/10000 [00:00<02:01, 81.87it/s, env_step=10015, len=1614, loss/actor=7462.664, loss/critic=316008.168, n/ep=0, n/st=1, rew=-3175363.81]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1: test_reward: -3167989.250000 ± 0.000000, best_reward: -1734.443604 ± 0.000000 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #2: 10001it [02:06, 79.05it/s, env_step=20000, len=1614, loss/actor=18602.580, loss/critic=1595386.015, n/ep=0, n/st=1, rew=-3169050.61]                           \n",
      "Epoch #3:   0%|          | 14/10000 [00:00<01:57, 85.28it/s, env_step=20014, len=1614, loss/actor=18666.871, loss/critic=1348980.447, n/ep=0, n/st=1, rew=-3169050.61]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #2: test_reward: -3167989.250000 ± 0.000000, best_reward: -1734.443604 ± 0.000000 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #3: 10001it [02:06, 79.22it/s, env_step=30000, len=1614, loss/actor=30573.313, loss/critic=1964113.387, n/ep=0, n/st=1, rew=-3161062.73]                           \n",
      "Epoch #4:   0%|          | 15/10000 [00:00<01:51, 89.74it/s, env_step=30015, len=1614, loss/actor=30694.356, loss/critic=2361068.220, n/ep=0, n/st=1, rew=-3161062.73]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #3: test_reward: -3167989.250000 ± 0.000000, best_reward: -1734.443604 ± 0.000000 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #4: 10001it [02:06, 79.35it/s, env_step=40000, len=1614, loss/actor=43216.449, loss/critic=7929119.890, n/ep=0, n/st=1, rew=-3168184.35]                           \n",
      "Epoch #5:   0%|          | 15/10000 [00:00<02:00, 82.96it/s, env_step=40015, len=1614, loss/actor=43413.108, loss/critic=9240920.619, n/ep=0, n/st=1, rew=-3168184.35]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #4: test_reward: -3167989.250000 ± 0.000000, best_reward: -1734.443604 ± 0.000000 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #5: 10001it [02:06, 78.78it/s, env_step=50000, len=1614, loss/actor=55289.766, loss/critic=8471543.328, n/ep=0, n/st=1, rew=-3179741.22]                           \n",
      "Epoch #6:   0%|          | 18/10000 [00:00<01:37, 102.59it/s, env_step=50018, len=1614, loss/actor=55468.651, loss/critic=10623230.201, n/ep=0, n/st=1, rew=-3179741.22]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #5: test_reward: -3167989.250000 ± 0.000000, best_reward: -1734.443604 ± 0.000000 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #6:  48%|####8     | 4803/10000 [00:58<01:03, 81.64it/s, env_step=54802, len=1614, loss/actor=60525.509, loss/critic=10525524.827, n/ep=0, n/st=1, rew=-3168994.47]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-7cbaaf7bf6cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;31m# trainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m result = offpolicy_trainer(\n\u001b[0m\u001b[1;32m     61\u001b[0m     \u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_collector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_collector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mstep_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_per_collect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_per_test\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tianshou/trainer/offpolicy.py\u001b[0m in \u001b[0;36moffpolicy_trainer\u001b[0;34m(policy, train_collector, test_collector, max_epoch, step_per_epoch, step_per_collect, episode_per_test, batch_size, update_per_step, train_fn, test_fn, stop_fn, save_fn, reward_metric, logger, verbose, test_in_train)\u001b[0m\n\u001b[1;32m    127\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdate_per_step\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"n/st\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m                     \u001b[0mgradient_step\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m                     \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_collector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m                         \u001b[0mstat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tianshou/policy/base.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, sample_size, buffer, **kwargs)\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_process_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tianshou/policy/modelfree/ddpg.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, batch, **kwargs)\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mBatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;31m# critic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m         td, critic_loss = self._mse_optimizer(\n\u001b[0m\u001b[1;32m    160\u001b[0m             batch, self.critic, self.critic_optim)\n\u001b[1;32m    161\u001b[0m         \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtd\u001b[0m  \u001b[0;31m# prio-buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tianshou/policy/modelfree/ddpg.py\u001b[0m in \u001b[0;36m_mse_optimizer\u001b[0;34m(batch, critic, optimizer)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0mcritic_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcritic_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'betas'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             F.adam(params_with_grad,\n\u001b[0m\u001b[1;32m    109\u001b[0m                    \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                    \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "resume_path = None\n",
    "\n",
    "seed = 1\n",
    "actor_lr = 1e-4\n",
    "critic_lr = 1e-3\n",
    "tau = .001\n",
    "gamma = .99\n",
    "exploration_noise = .15\n",
    "buffer_size = 100000\n",
    "batch_size = 256\n",
    "\n",
    "# seed\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "train_envs.seed(seed)\n",
    "test_envs.seed(seed)\n",
    "\n",
    "# model\n",
    "net_a = Net(state_shape, hidden_sizes=[256, 256], device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "actor = Actor(net_a, action_shape, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.cuda.is_available():\n",
    "    actor = actor.to('cuda')\n",
    "actor_optim = torch.optim.Adam(actor.parameters(), lr=actor_lr)\n",
    "\n",
    "net_c = Net(state_shape, action_shape,\n",
    "            hidden_sizes=[256, 256],\n",
    "            concat=True, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "critic = Critic(net_c, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.cuda.is_available():\n",
    "    critic = critic.to('cuda')\n",
    "critic_optim = torch.optim.Adam(critic.parameters(), lr=critic_lr)\n",
    "\n",
    "policy = DDPGPolicy(\n",
    "    actor, actor_optim, critic, critic_optim,\n",
    "    tau=tau, gamma=gamma,\n",
    "    exploration_noise=GaussianNoise(sigma=exploration_noise),\n",
    "    estimation_step=1, action_space=action_shape)\n",
    "\n",
    "# load a previous policy\n",
    "if resume_path:\n",
    "    policy.load_state_dict(torch.load(resume_path, map_location='cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "    print(\"Loaded agent from: \", resume_path)\n",
    "\n",
    "# collector\n",
    "\n",
    "buffer = ReplayBuffer(buffer_size)\n",
    "train_collector = Collector(policy, train_envs, buffer, exploration_noise=True)\n",
    "test_collector = Collector(policy, test_envs)\n",
    "train_collector.collect(n_step=buffer_size // 4, random=True)\n",
    "\n",
    "# log\n",
    "t0 = datetime.datetime.now().strftime(\"%m%d_%H%M%S\")\n",
    "log_file = f'seed_{seed}_{t0}_ddpg'\n",
    "log_path = os.path.join('log', 'ddpg')\n",
    "\n",
    "def save_fn(policy):\n",
    "    torch.save(policy.state_dict(), os.path.join(log_path, log_file + '_policy.pth'))\n",
    "    \n",
    "# trainer\n",
    "result = offpolicy_trainer(\n",
    "    policy, train_collector, test_collector, max_epoch=15,\n",
    "    step_per_epoch=10000, step_per_collect=1, episode_per_test= 1,\n",
    "    batch_size=batch_size, save_fn=save_fn,\n",
    "    update_per_step=1, test_in_train=False)\n",
    "\n",
    "# Let's watch its performance!\n",
    "# print('\\n start testing')\n",
    "# policy.eval()\n",
    "# test_envs.seed(seed)\n",
    "# test_collector.reset()\n",
    "# result = test_collector.collect(n_episode=1)\n",
    "# print(f'Final reward: {result[\"rews\"].mean()}, length: {result[\"lens\"].mean()}')\n",
    "\n",
    "#save policy\n",
    "# torch.save(policy.state_dict(), os.path.join(log_path, log_file + '_policy.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is for plotting the dynamic of reward during evaluation (not plotting now for this version)\n",
    "# needs to be modified corresponding when the reward function in the training part is changed.\n",
    "# ideally this process should be automatic (to update)\n",
    "def get_reward(acceleration, newSpacing, follower_speed, leader_speed):\n",
    "    newTimeGap = newSpacing / (follower_speed + .001)\n",
    "    expected_speed = 33.5\n",
    "    expected_time_gap = 1\n",
    "    alpha = 1\n",
    "    beta = 1\n",
    "    gamma = 1\n",
    "    delta = 4\n",
    "    reward = 0\n",
    "    \n",
    "    # time headway\n",
    "    penalty1 = [[0]]\n",
    "    if expected_time_gap > newTimeGap > 0:\n",
    "        penalty1 = - alpha * (100 - 100 * np.sqrt(max(0, expected_time_gap-(expected_time_gap-newTimeGap)**2)))\n",
    "        reward += penalty1\n",
    "\n",
    "    # speed reward\n",
    "    reward2 = beta * min(expected_speed, follower_speed)\n",
    "    reward += reward2\n",
    "\n",
    "    #speed diff reward\n",
    "    reward3 = [[0]]\n",
    "    if newTimeGap < expected_time_gap and leader_speed > follower_speed:\n",
    "        reward3 = gamma * (leader_speed - follower_speed) * (expected_time_gap - newTimeGap)\n",
    "        reward += reward3\n",
    "        \n",
    "    penalty4 = - delta * acceleration ** 2\n",
    "    reward += penalty4\n",
    "\n",
    "    return reward, penalty1, reward2, reward3, penalty4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "policy.load_state_dict(torch.load(os.getcwd() + '\\\\' + os.path.join(log_path, log_file + '_policy.pth'), \n",
    "                                  map_location='cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "print(\"Loaded agent from: \", log_file)\n",
    "policy.eval()\n",
    "rewardHist = [[np.nan] * 4]\n",
    "\n",
    "from CF_env import vehicle\n",
    "SIM_RESOLUTION = .1\n",
    "USED_HISTORY_STAMP  = 1\n",
    "leadSpeedProfile = np.genfromtxt('test_leader_high.csv', delimiter=',')\n",
    "leaderLoc = [0]\n",
    "followerLoc = [-30]\n",
    "for v in leadSpeedProfile[1:USED_HISTORY_STAMP]:\n",
    "    leaderLoc.append(leaderLoc[-1] + v * SIM_RESOLUTION)\n",
    "    followerLoc.append(followerLoc[-1] + v * SIM_RESOLUTION)\n",
    "follower = vehicle(leadSpeedProfile[:USED_HISTORY_STAMP], followerLoc)\n",
    "leader = vehicle(leadSpeedProfile[:USED_HISTORY_STAMP], leaderLoc)\n",
    "followerSpeedProfile = leadSpeedProfile[:USED_HISTORY_STAMP]\n",
    "followerSpacing = np.array([30] * USED_HISTORY_STAMP)\n",
    "observation = np.concatenate((leader.speedT, follower.speedT,\n",
    "                          [leader.locT[i] - follower.locT[i] for i in range(USED_HISTORY_STAMP)]))\n",
    "\n",
    "t = USED_HISTORY_STAMP\n",
    "while True:\n",
    "    with torch.no_grad():  \n",
    "        obs = np.array([observation])\n",
    "        result = policy.actor(obs)\n",
    "    act = to_numpy(result[0])\n",
    "    action = policy.map_action(act)\n",
    "    \n",
    "    follower.action_a(action)\n",
    "    leader.action_v(leadSpeedProfile[t])\n",
    "    newSpacing = leader.location - follower.location\n",
    "    newStates = np.concatenate((leader.speedT, follower.speedT,\n",
    "                          [leader.locT[i] - follower.locT[i] for i in range(USED_HISTORY_STAMP)]))\n",
    "    followerSpeedProfile = np.append(followerSpeedProfile, [follower.speed])\n",
    "    followerSpacing = np.append(followerSpacing, newSpacing)\n",
    "    reward, penalty1, reward2, reward3, penalty4 = get_reward(action, \n",
    "                                                              newSpacing, \n",
    "                                                              follower.speed, \n",
    "                                                              leader.speed)\n",
    "#     rewardHist.append([i[0][0] for i in [penalty1, reward2, reward3, penalty4]])\n",
    "    #current obs\n",
    "    t += 1\n",
    "    observation = newStates\n",
    "    if newSpacing < 0 or t >= len(leadSpeedProfile):\n",
    "        done = 1\n",
    "    else:\n",
    "        done = 0\n",
    "        \n",
    "    if done:\n",
    "        break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-08ae26aa4dcc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleadSpeedProfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleadSpeedProfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'leader'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfollowerSpeedProfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollowerSpeedProfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'follower'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time(s)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'speed(m/s)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "plt.figure(figsize = (10, 3))\n",
    "plt.plot(np.arange(len(leadSpeedProfile)) / 10, leadSpeedProfile, label = 'leader')\n",
    "plt.plot(np.arange(len(followerSpeedProfile)) / 10, followerSpeedProfile, label = 'follower')\n",
    "plt.xlabel('time(s)')\n",
    "plt.ylabel('speed(m/s)')\n",
    "# plt.ylim(22.5, 32.5)\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize = (10, 3))\n",
    "plt.plot(np.arange(len(followerSpeedProfile)) / 10, followerSpacing)\n",
    "plt.xlabel('time(s)')\n",
    "plt.ylabel('spacing(m)')\n",
    "plt.grid(True)\n",
    "# plt.ylim(20, 45)\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(round(min(followerSpeedProfile[:1000]),2), round(min(followerSpeedProfile[1000:]),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(os.path.join(log_path, log_file + 'trajInfo'), 'wb')\n",
    "pickle.dump([leadSpeedProfile, followerSpeedProfile, followerSpacing], file)\n",
    "file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
